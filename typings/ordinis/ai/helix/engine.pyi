"""
This type stub file was generated by pyright.
"""

import asyncio
from collections.abc import AsyncIterator
from dataclasses import dataclass
from typing import Any
from ordinis.ai.helix.config import HelixConfig
from ordinis.ai.helix.models import ChatMessage, ChatResponse, EmbeddingResponse, ModelInfo, ModelType
from ordinis.engines.base import BaseEngine, GovernanceHook

"""
Helix - Unified LLM Provider Engine.

Orchestrates LLM providers with caching, rate limiting, and fallback.
"""
_logger = ...
@dataclass
class CacheEntry:
    """Cached response entry."""
    response: ChatResponse | EmbeddingResponse
    timestamp: float
    hits: int = ...


@dataclass
class RateLimitState:
    """Rate limiter state."""
    request_times: list[float] = ...
    token_counts: list[tuple[float, int]] = ...
    semaphore: asyncio.Semaphore | None = ...


class Helix(BaseEngine[HelixConfig]):
    """
    Unified LLM provider for Ordinis.

    Provides:
    - Chat completions (sync and async)
    - Embeddings generation
    - Model routing with fallback
    - Response caching
    - Rate limiting
    - Retry with exponential backoff
    - Governance integration (preflight checks and auditing)
    """
    def __init__(self, config: HelixConfig | None = ..., governance_hook: GovernanceHook | None = ...) -> None:
        """
        Initialize Helix.

        Args:
            config: Configuration options. Uses defaults if None.
            governance_hook: Optional governance hook.
        """
        ...
    
    async def generate(self, messages: list[dict[str, str]] | list[ChatMessage], model: str | None = ..., temperature: float | None = ..., max_tokens: int | None = ..., stop: list[str] | None = ..., use_cache: bool | None = ..., **kwargs: Any) -> ChatResponse:
        """
        Generate chat completion.

        Args:
            messages: Conversation messages
            model: Model name/alias (uses default if None)
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            stop: Stop sequences
            use_cache: Override cache setting
            **kwargs: Additional provider options

        Returns:
            Chat completion response
        """
        ...
    
    async def generate_stream(self, messages: list[dict[str, str]] | list[ChatMessage], model: str | None = ..., temperature: float | None = ..., max_tokens: int | None = ..., **kwargs: Any) -> AsyncIterator[str]:
        """
        Generate streaming chat completion.

        Args:
            messages: Conversation messages
            model: Model name/alias
            temperature: Sampling temperature
            max_tokens: Maximum tokens to generate
            **kwargs: Additional provider options

        Yields:
            Content chunks as they're generated
        """
        ...
    
    async def embed(self, texts: str | list[str], model: str | None = ..., use_cache: bool | None = ...) -> EmbeddingResponse:
        """
        Generate embeddings.

        Args:
            texts: Text(s) to embed
            model: Embedding model name/alias
            use_cache: Override cache setting

        Returns:
            Embedding vectors
        """
        ...
    
    def list_models(self, model_type: ModelType | None = ...) -> list[ModelInfo]:
        """List available models."""
        ...
    
    def get_metrics(self) -> dict[str, Any]:
        """Get usage metrics."""
        ...
    
    async def health_check(self) -> dict[str, bool]:
        """Check health of all providers."""
        ...
    
    def generate_sync(self, messages: list[dict[str, str]] | list[ChatMessage], model: str | None = ..., **kwargs: Any) -> ChatResponse:
        """Synchronous chat completion."""
        ...
    
    def embed_sync(self, texts: str | list[str], model: str | None = ...) -> EmbeddingResponse:
        """Synchronous embedding generation."""
        ...
    


