"""
This type stub file was generated by pyright.
"""

from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any

"""
Helix data models.

Defines request/response structures for LLM interactions.
"""
class ModelType(Enum):
    """Model capability type."""
    CHAT = ...
    EMBEDDING = ...
    CODE = ...
    MULTIMODAL = ...


class ProviderType(Enum):
    """LLM provider backend."""
    NVIDIA_API = ...
    NVIDIA_NIM = ...
    MISTRAL_API = ...
    OPENAI_API = ...
    AZURE_OPENAI_API = ...
    LOCAL_GPU = ...
    LOCAL_CPU = ...
    MOCK = ...


@dataclass(frozen=True)
class ModelInfo:
    """Model metadata and capabilities."""
    model_id: str
    display_name: str
    model_type: ModelType
    provider: ProviderType
    context_length: int
    embedding_dim: int | None = ...
    supports_streaming: bool = ...
    supports_function_calling: bool = ...
    cost_per_1k_tokens: float = ...
    default_temperature: float = ...
    max_output_tokens: int = ...
    def __str__(self) -> str:
        ...
    


@dataclass
class ChatMessage:
    """Single message in a conversation."""
    role: str
    content: str
    name: str | None = ...
    metadata: dict[str, Any] = ...
    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for API calls."""
        ...
    


@dataclass
class UsageInfo:
    """Token usage information."""
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int
    cost_usd: float = ...
    @classmethod
    def empty(cls) -> UsageInfo:
        """Create empty usage info."""
        ...
    


@dataclass
class ChatResponse:
    """Response from chat completion."""
    content: str
    model: str
    provider: ProviderType
    usage: UsageInfo
    finish_reason: str = ...
    latency_ms: float = ...
    timestamp: datetime = ...
    raw_response: dict[str, Any] | None = ...
    cached: bool = ...
    def __str__(self) -> str:
        ...
    


@dataclass
class EmbeddingResponse:
    """Response from embedding generation."""
    embeddings: list[list[float]]
    model: str
    provider: ProviderType
    usage: UsageInfo
    dimension: int
    latency_ms: float = ...
    timestamp: datetime = ...
    cached: bool = ...
    @property
    def count(self) -> int:
        """Number of embeddings returned."""
        ...
    
    def as_numpy(self) -> Any:
        """Convert to numpy array (requires numpy)."""
        ...
    


class HelixError(Exception):
    """Base exception for Helix errors."""
    def __init__(self, message: str, provider: ProviderType | None = ..., model: str | None = ..., retriable: bool = ...) -> None:
        ...
    


class RateLimitError(HelixError):
    """Rate limit exceeded."""
    def __init__(self, message: str = ..., retry_after: float | None = ..., **kwargs: Any) -> None:
        ...
    


class ModelNotFoundError(HelixError):
    """Requested model not available."""
    def __init__(self, model: str, available: list[str] | None = ..., **kwargs: Any) -> None:
        ...
    


class ProviderError(HelixError):
    """Provider-specific error."""
    def __init__(self, message: str, status_code: int | None = ..., **kwargs: Any) -> None:
        ...
    


